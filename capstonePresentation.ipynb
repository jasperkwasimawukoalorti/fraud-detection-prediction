{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JzvCz-zzdJA"
      },
      "source": [
        "### **1. Data Understanding & Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSu4DC-vzAmM"
      },
      "source": [
        "installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voGUkMlcxu8l",
        "outputId": "706fb0dc-7efd-472a-f729-71f55736644b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: NumPy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: Scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: Streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: Matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: Seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from Pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from Pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from Pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from Scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from Scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (8.3.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from Streamlit) (6.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (3.2.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->Streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->Pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->Streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->Streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->Streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->Streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->Streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->Streamlit) (0.29.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install Pandas NumPy Scikit-learn Streamlit Matplotlib Seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQSHjCZ11IZa"
      },
      "source": [
        "1.1. Load and Inspect the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxCJdVG9z19Y"
      },
      "source": [
        "importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "IP_oWtVxz2x0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuMD4j9V0UqR"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Assuming the file 'fraud_data.csv' has been downloaded and saved locally\n",
        "df = pd.read_csv('fraud_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9M5b56e0a_L"
      },
      "outputs": [],
      "source": [
        "# Inspect the dataset\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnyqokba7t-e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nDataset Information:\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iztaAIn67wml"
      },
      "outputs": [],
      "source": [
        "# satistical discription of the dataset\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZEP8hs9702E"
      },
      "outputs": [],
      "source": [
        "# show the nunber ofrows and columns\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC-rl-2F06Y9"
      },
      "source": [
        "1.2. Handle Missing Values and Remove Duplicates\n",
        "\n",
        "Check for missing values and duplicates, and clean the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAzcBayX0ycf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Handle missing values\n",
        "print(\"\\nMissing values count per column:\")\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGu8bRZ25oDq"
      },
      "source": [
        "If there are missing values, a common approach is to drop rows or impute.\n",
        "\n",
        "For simplicity, we'll assume no critical missing data based on common fraud datasets.\n",
        "\n",
        "If imputation is needed: df['column'].fillna(df['column'].median(), inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1H2djdY_PfG"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNZNYHGH_XMA"
      },
      "outputs": [],
      "source": [
        "# Re-Checking for missing values after cleaning data\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhzZAGYL1hwT"
      },
      "outputs": [],
      "source": [
        "# Remove duplicates\n",
        "print(f\"\\nNumber of duplicate rows before removal: {df.duplicated().sum()}\")\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Number of duplicate rows after removal: {df.duplicated().sum()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3lQLbnI1p2v"
      },
      "source": [
        "1.3. Encode Categorical Variables.\n",
        "\n",
        "The type column is categorical and must be converted to a numerical format for the model.\n",
        "\n",
        "One-Hot Encoding is suitable here, or for simplicity in the prediction app, we can use Label Encoding and retain the mapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7flKF8jU1xKh"
      },
      "outputs": [],
      "source": [
        "# We'll use One-Hot Encoding for 'type' as it's nominal data\n",
        "df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siQ_8eyuBvSC"
      },
      "outputs": [],
      "source": [
        "# Drop columns not needed for modeling (e.g., 'nameOrig', 'nameDest', 'isFlaggedFraud')\n",
        "\n",
        "df.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1, inplace=True, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGIu8Fg815uH"
      },
      "outputs": [],
      "source": [
        "# Save the column order for the Streamlit app later\n",
        "model_features = df.drop('isFraud', axis=1).columns.tolist()\n",
        "print(\"\\nFeatures after encoding and dropping columns:\")\n",
        "print(model_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBnJ2BY-1-a-"
      },
      "source": [
        "1.4. Scale Numerical Features.\n",
        "\n",
        "Scale numerical features like amount, oldbalanceOrg, and others.\n",
        "\n",
        "Scaling prevents features with larger values from dominating the model training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZcl8m3g2EP-"
      },
      "outputs": [],
      "source": [
        "# Identify numerical columns (excluding the one-hot encoded 'type' columns and the target)\n",
        "numerical_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW9hYFzp2JGP"
      },
      "outputs": [],
      "source": [
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gcnaF032OMZ"
      },
      "outputs": [],
      "source": [
        "# Apply the scaler to the numerical columns\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ssOs2ZP2TWu"
      },
      "outputs": [],
      "source": [
        "# Save the scaler for use in the Streamlit app\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"\\nNumerical features scaled. Scaler saved as 'scaler.pkl'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TQVP9fb2beG"
      },
      "source": [
        "1.5. Explore Fraud Patterns Using Charts.\n",
        "\n",
        "Visualize the data to understand the distribution of the target variable and fraud patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYrEEz7N2j4Z"
      },
      "outputs": [],
      "source": [
        "## Target Variable Distribution (Fraud vs. Not Fraud)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='isFraud', data=df)\n",
        "plt.title('Distribution of the Target Variable (isFraud)')\n",
        "plt.xlabel('Is Fraud (0: No Fraud, 1: Fraud)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.xticks([0, 1], ['No Fraud', 'Fraud'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkgm_Ole2t1z"
      },
      "outputs": [],
      "source": [
        "### Fraud by Transaction Type\n",
        "# The one-hot encoded columns are type_CASH_OUT, type_DEBIT, type_PAYMENT, type_TRANSFER\n",
        "type_cols = [col for col in df.columns if col.startswith('type_')]\n",
        "fraud_by_type = df.groupby('isFraud')[type_cols].sum()\n",
        "\n",
        "fraud_by_type.T.plot(kind='bar', figsize=(8, 6))\n",
        "plt.title('Fraudulent Transactions by Transaction Type')\n",
        "plt.xlabel('Transaction Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Is Fraud', labels=['No Fraud', 'Fraud'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXYq573S2inv"
      },
      "source": [
        "\n",
        "\n",
        "### 2. Machine Learning Model Development\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZEcawAq3FBE"
      },
      "source": [
        "#### 2.1. Select Features and Split Data\n",
        "\n",
        "Select the feature columns (`X`) and the target column (`y`), then split the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wixZC_M23LKb"
      },
      "outputs": [],
      "source": [
        "# Define features (X) and target (y)\n",
        "X = df.drop('isFraud', axis=1)\n",
        "y = df['isFraud']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztUAb4uM3PvA"
      },
      "outputs": [],
      "source": [
        "# Split the data (80% train, 20% test is a common split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nX_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NboyDW8l3UJt"
      },
      "source": [
        "2.2. Train a Random Forest Model\n",
        "Train the recommended Random Forest Classifier. Given the high class imbalance in typical fraud datasets, adding class_weight='balanced' can often improve performance by penalizing errors in the minority class (Fraud).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKEXzxJZCO8X"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nRandom Forest Model Training Complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jSXLn4K3joY"
      },
      "source": [
        "2.3. Evaluate Model Performance\n",
        "Evaluate the model using accuracy, precision, recall, and F1-score. For fraud detection, Precision and Recall are often more critical than Accuracy due to the extreme class imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIUkjEVk3own"
      },
      "outputs": [],
      "source": [
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-4yMzdN3wYG"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nModel Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")   # Aim for 90%+\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHBxLQDl38nr"
      },
      "outputs": [],
      "source": [
        "# Visualize the Confusion Matrix for a complete picture\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Not Fraud (0)', 'Fraud (1)'],\n",
        "            yticklabels=['Not Fraud (0)', 'Fraud (1)'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82VUl8h44FA2"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Save the trained model for deployment\n",
        "joblib.dump(model, 'random_forest_model.pkl')\n",
        "print(\"\\nTrained model saved as 'random_forest_model.pkl'.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
